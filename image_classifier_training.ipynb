{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uo9DV6SrYt4j",
   "metadata": {
    "id": "uo9DV6SrYt4j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e2d28e-73b5-4e72-87c0-7f35560054fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up augmentation class\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "class MyTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.angles = [-30, -15, 0, 15, 30]\n",
    "        self.brightness_levels = [0.9, 1.1, 1.2]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        x = TF.rotate(x, angle)\n",
    "        x = TF.adjust_brightness(x, random.choice(self.brightness_levels))\n",
    "        return x\n",
    "\n",
    "tranformation = MyTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad7d2cb9-34bc-4ff2-856e-d4586267fc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, Callable\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_frame: pd.DataFrame, path_to_slides: str, image_size: Tuple[int, int],\n",
    "                transformation_fn: Callable = None, pseudo_epoch_length=1000, mode=\"\"):\n",
    "        super().__init__()\n",
    "        self._image_size = image_size\n",
    "\n",
    "        self._resize = transforms.Resize(size=image_size, antialias=True)\n",
    "        \n",
    "        self.annotations_frame = annotation_frame\n",
    "        self.mode = mode\n",
    "        if not os.path.isdir(path_to_slides):\n",
    "            raise IOError(\"Image path is not set correctly\")\n",
    "            \n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.pseudo_epoch_length = pseudo_epoch_length\n",
    "        \n",
    "        self.transformation = transformation_fn\n",
    "        self.transform_to_tensor = transforms.ToTensor()\n",
    "        self.slide_list, self.annotation_list = self._initialize()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_list)\n",
    "\n",
    "        \n",
    "    def _initialize(self):\n",
    "        \"\"\"\n",
    "        Initilize the internal dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        slide_list = []\n",
    "        annotation_list = []\n",
    "        for _, row in self.annotations_frame.iterrows():\n",
    "            image = row['filename']\n",
    "            label = row['label']\n",
    "            open_image = Image.open(os.path.join(self.path_to_slides, image)).convert('RGB')\n",
    "            open_image = self.transform_to_tensor(open_image)\n",
    "            if self.mode == \"train\":\n",
    "                if label == 1:\n",
    "                    # Undersample class 1 with a probability of 25%\n",
    "                    if random.choice([1, 2, 3, 4]) == 1:\n",
    "                        continue\n",
    "                    slide_list.append(open_image)\n",
    "                    annotation_list.append(label)\n",
    "                # Oversample classes with label 3 using transformations\n",
    "                elif label == 3:\n",
    "                        \n",
    "                    slide_list.append(open_image)\n",
    "                    annotation_list.append(label)\n",
    "\n",
    "                    slide_list.append(open_image)\n",
    "                    annotation_list.append(label)\n",
    "\n",
    "\n",
    "                elif label in [4, 5]:\n",
    "                    for i in range(3):\n",
    "                        slide_list.append(open_image)\n",
    "                        annotation_list.append(label)\n",
    "\n",
    "                elif label in [0, 2]:\n",
    "\n",
    "                    slide_list.append(open_image)\n",
    "                    annotation_list.append(label)\n",
    "                    \n",
    "    \n",
    "                elif label in  [9 , 10, 11]:\n",
    "                    for i in range(20):\n",
    "                        slide_list.append(open_image)\n",
    "                        annotation_list.append(label)\n",
    "                else:\n",
    "                    for i in range(15):\n",
    "                        slide_list.append(open_image)\n",
    "                        annotation_list.append(label)\n",
    "            else:\n",
    "                slide_list.append(open_image)\n",
    "                annotation_list.append(label)\n",
    "                    \n",
    "        \n",
    "        print(f\"The counts after adjustment: {collections.Counter(annotation_list)}\")\n",
    "        print(f\"The number of images in the dataset : {len(annotation_list)}\")\n",
    "\n",
    "\n",
    "        return slide_list, annotation_list\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"\n",
    "        Load an image\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.slide_list[index]\n",
    "        label = self.annotation_list[index]\n",
    "\n",
    "        # Perform necessary conversions\n",
    "        img = self._resize.forward(img)\n",
    "        img = self.transformation(img) if self.transformation else img\n",
    "        \n",
    "           \n",
    "        return img.to(torch.device(\"cuda\")), torch.tensor(label, device=torch.device(\"cuda\"), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brYRbF8idhWa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brYRbF8idhWa",
    "outputId": "ec0724fe-c855-4524-e622-2df6edc43981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1     6522\n",
      "0     3569\n",
      "2     3202\n",
      "3     1302\n",
      "4      587\n",
      "5      285\n",
      "6      154\n",
      "7       62\n",
      "8       38\n",
      "11      23\n",
      "9       22\n",
      "10      21\n",
      "Name: count, dtype: int64\n",
      "The counts after adjustment: Counter({1: 4889, 0: 3569, 2: 3202, 3: 2604, 6: 2310, 4: 1761, 7: 930, 5: 855, 8: 570, 11: 460, 9: 440, 10: 420})\n",
      "The number of images in the dataset : 22010\n",
      "The counts after adjustment: Counter({1: 1630, 0: 892, 2: 801, 3: 326, 4: 147, 5: 71, 6: 39, 7: 16, 8: 9, 9: 6, 10: 5, 11: 5})\n",
      "The number of images in the dataset : 3947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "\n",
    "annotation_frames = pd.read_pickle(\"annotations.p\")\n",
    "\n",
    "# Stratified sampling\n",
    "\n",
    "df_train, df_test = train_test_split(annotation_frames, test_size=0.2, stratify=annotation_frames[[\"label\"]])\n",
    "\n",
    "print(df_train['label'].value_counts())\n",
    "# Augmentation function\n",
    "\n",
    "train_dataset = Dataset(path_to_slides=\"/home/ESPL_001/user/Downloads/fourth_milestore/crops\", image_size=(32, 32), annotation_frame=df_train, transformation_fn=tranformation, mode=\"train\")\n",
    "test_dataset = Dataset(path_to_slides=\"/home/ESPL_001/user/Downloads/fourth_milestore/crops\", image_size=(32, 32), annotation_frame=df_test, mode=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e945d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_laoder = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivwojB899YXw",
   "metadata": {
    "id": "ivwojB899YXw"
   },
   "source": [
    "#1. Implement a Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WeAfxZSy6N6N",
   "metadata": {
    "id": "WeAfxZSy6N6N"
   },
   "source": [
    "#2. Develop a Custom Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    " \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=(2,2), stride=2, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    " \n",
    "        self.flat = nn.Flatten()\n",
    " \n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(0.3)\n",
    " \n",
    "        self.fc5 = nn.Linear(512, 12)\n",
    "  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input 3x32x32, output 32x32x32\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.drop1(x)\n",
    "        # input 32x32x32, output 32x32x32\n",
    "        x = self.act2(self.conv2(x))\n",
    "        # input 32x32x32, output 32x16x16\n",
    "        x = self.pool2(x)\n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        # input 32x16x16, output 8192\n",
    "        x = self.flat(x)\n",
    "        # input 8192, output 512\n",
    "        x = self.act4(self.fc4(x))\n",
    "        x = self.drop4(x)\n",
    "        # input 512, output 12\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d872649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3647fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:12<00:00, 345.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: model accuracy 44.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 385.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: model accuracy 53.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 385.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: model accuracy 52.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 385.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: model accuracy 55.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 383.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: model accuracy 62.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 382.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: model accuracy 64.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 379.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: model accuracy 65.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 387.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: model accuracy 67.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 388.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: model accuracy 66.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4402/4402 [00:11<00:00, 393.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: model accuracy 66.33%\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model.to(torch.device(\"cuda\"))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(torch.device(\"cuda\"))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, labels in tqdm(train_laoder):\n",
    "        # forward, backward, and then weight update\n",
    "        y_pred = model(inputs)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        y_pred = model(inputs)\n",
    "        acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
    "        count += len(labels)\n",
    "    acc /= count\n",
    "    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b1207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "def find_image_size_after_kernel(image_size, filter_size, stride=1, padding=1):\n",
    "    return ((image_size - filter_size + 2*padding)/stride) + 1\n",
    "\n",
    "print(find_image_size_after_kernel(36, 2, padding=0, stride=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIrT1Qsg6ben",
   "metadata": {
    "id": "xIrT1Qsg6ben"
   },
   "source": [
    "#4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55adf8b4-6228-4ab7-ac77-f2043647af9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testset Accuracy(mean): 69.318470 %\n",
      "\n",
      "Confusion Matirx : \n",
      "[[ 758  112    2    3    4    0   12    1    0    0    0    0]\n",
      " [  46 1476   53   30    4    1   17    2    0    1    0    0]\n",
      " [  25  280  372   79   10    3   28    1    1    2    0    0]\n",
      " [  11   81   78   89   19    2   35    5    0    3    3    0]\n",
      " [   4   24   12   48   16    3   34    1    0    4    1    0]\n",
      " [   3    6    4   20    3    3   28    2    0    1    0    1]\n",
      " [   0    3    0    7    1    3   14    8    0    1    1    1]\n",
      " [   1    1    1    1    1    0    6    4    0    0    0    1]\n",
      " [   0    1    0    0    1    1    2    1    1    1    0    1]\n",
      " [   0    0    1    0    0    0    3    0    0    2    0    0]\n",
      " [   0    0    0    0    0    0    2    0    0    0    1    2]\n",
      " [   3    1    0    0    0    0    0    0    0    0    1    0]]\n",
      "- Sensitivity :  96.97766097240473\n",
      "- Specificity :  87.1264367816092\n",
      "- Precision:  92.9471032745592\n",
      "- NPV:  94.27860696517413\n",
      "- F1 :  94.91961414790998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classes = [x for x in range(12)]\n",
    "\n",
    "\n",
    "CM=0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(images) #file_name\n",
    "        preds = torch.argmax(outputs.data, 1)\n",
    "        CM+=confusion_matrix(labels.cpu(), preds.cpu(),labels=classes)\n",
    "\n",
    "    tn=CM[0][0]\n",
    "    tp=CM[1][1]\n",
    "    fp=CM[0][1]\n",
    "    fn=CM[1][0]\n",
    "    acc=np.sum(np.diag(CM)/np.sum(CM))\n",
    "    sensitivity=tp/(tp+fn)\n",
    "    precision=tp/(tp+fp)\n",
    "\n",
    "    print('\\nTestset Accuracy(mean): %f %%' % (100 * acc))\n",
    "    print()\n",
    "    print('Confusion Matirx : ')\n",
    "    print(CM)\n",
    "    print('- Sensitivity : ',(tp/(tp+fn))*100)\n",
    "    print('- Specificity : ',(tn/(tn+fp))*100)\n",
    "    print('- Precision: ',(tp/(tp+fp))*100)\n",
    "    print('- NPV: ',(tn/(tn+fn))*100)\n",
    "    print('- F1 : ',((2*sensitivity*precision)/(sensitivity+precision))*100)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abae0766-7da9-4ff8-96a7-26ca875e97d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: 0 is 84.9 %\n",
      "Accuracy for class: 1 is 90.5 %\n",
      "Accuracy for class: 2 is 46.4 %\n",
      "Accuracy for class: 3 is 27.2 %\n",
      "Accuracy for class: 4 is 10.8 %\n",
      "Accuracy for class: 5 is 4.2 %\n",
      "Accuracy for class: 6 is 35.0 %\n",
      "Accuracy for class: 7 is 23.5 %\n",
      "Accuracy for class: 8 is 10.0 %\n",
      "Accuracy for class: 9 is 28.6 %\n",
      "Accuracy for class: 10 is 16.7 %\n",
      "Accuracy for class: 11 is 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / (total_pred[classname] + 1)\n",
    "    print(f'Accuracy for class: {classname} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "cbc2ccb7899380f8e39344ba6fff5f38db9a0526ef90eff1147cd398f3029ddf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
